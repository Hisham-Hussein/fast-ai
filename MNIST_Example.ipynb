{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading in the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data tuples:  50000\n",
      "Data points per input value:  784\n",
      "Output array size:  10\n",
      "Number of validation data tuples:  10000\n"
     ]
    }
   ],
   "source": [
    "print \"Number of training data tuples: \", len(training_data)\n",
    "print \"Data points per input value: \", len(training_data[0][0])\n",
    "print \"Output array size: \", len(training_data[0][1])\n",
    "print \"Number of validation data tuples: \",len(validation_data)\n",
    "print \"Number of test data tuples: \",len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a Network with 30 hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import network\n",
    "net = network.Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neurons in the respective layers:  [784, 30, 10] \n",
      "\n",
      "Number of biases between 1st and 2nd layers:  30\n",
      "Some example biases between 1st and 2nd layers: \n",
      "[[-0.65616326]\n",
      " [-0.66219892]\n",
      " [ 0.52779785]]\n",
      "Number of biases between 2nd and 3rd layers:  30\n",
      "Some example biases between 2nd and 3rd layers: \n",
      "[[ 0.11582079]\n",
      " [ 0.17365875]\n",
      " [-0.02760088]] \n",
      "\n",
      "Number of weights between 1st and 2nd layers:  30\n",
      "Some example weights between 1st and 2nd layers: \n",
      "[[ 0.0431288  -1.39348301 -0.38850388]\n",
      " [-0.98134354 -0.57518265 -0.21679014]\n",
      " [ 0.26499918  0.8704585  -1.14835446]]\n",
      "Number of weights between 2nd and 3rd layers:  10\n",
      "Some example weights between 2nd and 3rd layers: \n",
      "[[-1.01367654  0.18505978 -0.60359269]\n",
      " [-1.02634188  0.63877486 -0.90054889]\n",
      " [ 1.16044398  0.22333662 -0.95617607]]\n"
     ]
    }
   ],
   "source": [
    "print \"Number of neurons in the respective layers: \", net.sizes, \"\\n\"\n",
    "print \"Number of biases between 1st and 2nd layers: \", len(net.biases[0])\n",
    "print \"Some example biases between 1st and 2nd layers: \\n\", net.biases[0][:3]\n",
    "print \"Number of biases between 2nd and 3rd layers: \", len(net.biases[0])\n",
    "print \"Some example biases between 2nd and 3rd layers: \\n\", net.biases[1][:3], \"\\n\"\n",
    "print \"Number of weights between 1st and 2nd layers: \", len(net.weights[0])\n",
    "print \"Some example weights between 1st and 2nd layers: \\n\", net.weights[0][:3,:3]\n",
    "print \"Number of weights between 2nd and 3rd layers: \", len(net.weights[1])\n",
    "print \"Some example weights between 2nd and 3rd layers: \\n\", net.weights[1][:3,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use stochastic gradient descent to learn from the MNIST training_data over 10 epochs, with a mini-batch size of 10, and a learning rate of η=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9343 / 10000\n",
      "Epoch 1: 9383 / 10000\n",
      "Epoch 2: 9393 / 10000\n",
      "Epoch 3: 9404 / 10000\n",
      "Epoch 4: 9419 / 10000\n",
      "Epoch 5: 9435 / 10000\n",
      "Epoch 6: 9425 / 10000\n",
      "Epoch 7: 9458 / 10000\n",
      "Epoch 8: 9432 / 10000\n",
      "Epoch 9: 9433 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 10, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once we've trained a network it can be run very quickly indeed, on almost any computing platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the above experiment, changing the number of hidden neurons to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6178 / 10000\n",
      "Epoch 1: 7596 / 10000\n",
      "Epoch 2: 7687 / 10000\n",
      "Epoch 3: 7696 / 10000\n",
      "Epoch 4: 7709 / 10000\n",
      "Epoch 5: 7745 / 10000\n",
      "Epoch 6: 7722 / 10000\n",
      "Epoch 7: 7791 / 10000\n",
      "Epoch 8: 7825 / 10000\n",
      "Epoch 9: 8600 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = network.Network([784, 100, 10])\n",
    "net.SGD(training_data, 10, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should improve results, but there is quite some variation in results for this experiment, and some training runs give results quite a bit worse. Using the techniques introduced in chapter 3 will greatly reduce the variation in performance across different training runs for our networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of epochs of training, the mini-batch size, and the learning rate, η are known as hyper-parameters for our neural network, in order to distinguish them from the parameters (weights and biases) learnt by our learning algorithm. If we choose our hyper-parameters poorly, we can get bad results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, debugging a neural network can be challenging. This is especially true when the initial choice of hyper-parameters produces results no better than random noise. We might worry not only about the learning rate, but about every other aspect of our neural network. We might wonder if we've initialized the weights and biases in a way that makes it hard for the network to learn? Or maybe we don't have enough training data to get meaningful learning? Perhaps we haven't run for enough epochs? Or maybe it's impossible for a neural network with this architecture to learn to recognize handwritten digits? Maybe the learning rate is too low? Or, maybe, the learning rate is too high? When you're coming to a problem for the first time, you're not always sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lesson to take away from this is that debugging a neural network is not trivial, and, just as for ordinary programming, there is an art to it. You need to learn that art of debugging in order to get good results from neural networks. More generally, we need to develop heuristics for choosing good hyper-parameters and a good architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
